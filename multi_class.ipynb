{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125960ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device: Using CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "# Check GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print(\"GPU Device:\", device_name if device_name else \"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for save and load in python/json objects the dictionaries\n",
    "def save_pickle(dic, path):\n",
    "    with open(f\"{path}\", 'wb') as f:\n",
    "        pickle.dump(dic, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(f\"{path}\", 'rb',) as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# create a dataframe with id of the images without extensions (.jpg)\n",
    "def create_df():\n",
    "    name = []\n",
    "    mask = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH): # given a directory iterates over the files\n",
    "        for filename in filenames:\n",
    "            f = filename.split('.')[0]\n",
    "            name.append(f)\n",
    "\n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name))).sort_values('id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f49de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for images and masks\n",
    "IMAGE_PATH = r\"C:\\Users\\Rudra Thakar\\Downloads\\archive (5)\\classes_dataset\\classes_dataset\\original_images\"\n",
    "MASK_PATH  = r\"C:\\Users\\Rudra Thakar\\Downloads\\archive (5)\\classes_dataset\\classes_dataset\\label_images_semantic\"\n",
    "\n",
    "X = create_df()['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a36b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size   :  300\n",
      "Val Size     :  40\n",
      "Test Size    :  60\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "X_train, X_val = train_test_split(X, test_size=0.25, random_state=123)\n",
    "X_test, X_val = train_test_split(X_val, test_size=0.4, random_state=123) # array of indexes\n",
    "\n",
    "print('Train Size   : ', len(X_train))\n",
    "print('Val Size     : ', len(X_val))\n",
    "print('Test Size    : ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526d5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDatasetTF(Sequence):\n",
    "\n",
    "    def __init__(self, img_path, mask_path, X, transform=None, batch_size=8):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_ids = self.X[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        for id in batch_ids:\n",
    "            image = np.array(Image.open(self.img_path + id + '.png'))\n",
    "            mask = np.array(Image.open(self.mask_path + id + '.png'))\n",
    "\n",
    "            if self.transform is not None:\n",
    "                aug = self.transform(image=image, mask=mask)\n",
    "                image = aug['image']\n",
    "                mask = aug['mask']\n",
    "\n",
    "            norm = A.Normalize()(image=image, mask=np.expand_dims(mask, 0))\n",
    "\n",
    "            image = norm['image'].astype(np.float32) / 255.0\n",
    "            image = image.transpose(2, 0, 1)  # (C, H, W) â†’ same layout if needed for model\n",
    "\n",
    "            images.append(image)\n",
    "            masks.append(norm['mask'].astype(np.float32))\n",
    "\n",
    "        return np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0228bc0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mA\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_transforms \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      4\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m      5\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(brightness_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, contrast_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     11\u001b[0m valid_transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# no augmentation on validation/test\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.GaussNoise(),\n",
    "    A.GridDistortion(p=0.2),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.4)\n",
    "])\n",
    "\n",
    "valid_transforms = None  # no augmentation on validation/test\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "train_dataset = DroneDatasetTF(IMAGE_PATH, MASK_PATH, X_train, transform=train_transforms, batch_size=batch_size)\n",
    "valid_dataset = DroneDatasetTF(IMAGE_PATH, MASK_PATH, X_val, transform=valid_transforms, batch_size=batch_size)\n",
    "test_dataset = DroneDatasetTF(IMAGE_PATH, MASK_PATH, X_test, transform=valid_transforms, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa3681",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at some samples\n",
    "image, mask = train_dataset[0]\n",
    "\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.array(image).transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.array(mask).squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
    "plt.show()\n",
    "\n",
    "image, mask = valid_dataset[0]\n",
    "\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.array(image).transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.array(mask).squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
    "plt.show()\n",
    "\n",
    "image, mask = test_dataset[0]\n",
    "\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.array(image).transpose(1, 2, 0)) # for visualization we have to transpose back to HWC\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.array(mask).squeeze())  # for visualization we have to remove 3rd dimension of mask\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88828250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class DroneModelTF(Model):\n",
    "    def __init__(self, model, criterion):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.train_iou = tf.keras.metrics.Mean(name='train_iou')\n",
    "        self.val_iou = tf.keras.metrics.Mean(name='val_iou')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.model(inputs, training=training)\n",
    "\n",
    "    def compute_iou(self, y_true, y_pred, num_classes=5):\n",
    "        # y_true shape: (batch, h, w, 1) or (batch, h, w)\n",
    "        # y_pred shape: (batch, h, w, num_classes)\n",
    "        \n",
    "        y_pred_labels = tf.argmax(y_pred, axis=-1)\n",
    "        y_pred_labels = tf.expand_dims(y_pred_labels, axis=-1)  # shape (batch, h, w, 1)\n",
    "\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "        # Calculate intersection and union per class\n",
    "        ious = []\n",
    "        for i in range(num_classes):\n",
    "            true_class = tf.equal(y_true, i)\n",
    "            pred_class = tf.equal(y_pred_labels, i)\n",
    "            intersection = tf.reduce_sum(tf.cast(tf.logical_and(true_class, pred_class), tf.float32))\n",
    "            union = tf.reduce_sum(tf.cast(tf.logical_or(true_class, pred_class), tf.float32))\n",
    "            iou = tf.cond(tf.equal(union, 0), lambda: tf.constant(1.0), lambda: intersection / union)\n",
    "            ious.append(iou)\n",
    "        return tf.reduce_mean(ious)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, masks = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.criterion(masks, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        iou = self.compute_iou(masks, predictions)\n",
    "\n",
    "        self.compiled_metrics.update_state(masks, predictions)\n",
    "        self.train_iou.update_state(iou)\n",
    "\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['train_iou'] = self.train_iou.result()\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        images, masks = data\n",
    "        predictions = self(images, training=False)\n",
    "        loss = self.criterion(masks, predictions)\n",
    "\n",
    "        iou = self.compute_iou(masks, predictions)\n",
    "\n",
    "        self.compiled_metrics.update_state(masks, predictions)\n",
    "        self.val_iou.update_state(iou)\n",
    "\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['val_iou'] = self.val_iou.result()\n",
    "        results['loss'] = loss\n",
    "        return results\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.train_iou.reset_states()\n",
    "        self.val_iou.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420634c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models as sm\n",
    "\n",
    "arch = 'unet'\n",
    "enc_name = 'efficientnetb0'\n",
    "classes = 5\n",
    "input_shape = (None, None, 3)  # Change None if fixed size (height, width)\n",
    "\n",
    "# Create model\n",
    "model = sm.Unet(\n",
    "    backbone_name=enc_name,\n",
    "    input_shape=input_shape,\n",
    "    classes=classes,\n",
    "    activation=None,  # output logits, no activation for loss with from_logits=True\n",
    "    encoder_weights='imagenet'\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Dice Loss (multiclass)\n",
    "# segmentation_models has DiceLoss for multiclass, you can use this:\n",
    "loss = sm.losses.DiceLoss(class_weights=None, from_logits=True, mode='multiclass')\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[sm.metrics.IOUScore(threshold=0.5)])\n",
    "\n",
    "# ModelCheckpoint callback\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f'./checkpoints_{arch}/' + arch + '_best.h5',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ed7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=2,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
